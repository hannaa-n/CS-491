{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hannaa-n/CS-491/blob/main/Medical_Questions_Nguyen_Nguyen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28868e6f",
      "metadata": {
        "id": "28868e6f"
      },
      "source": [
        "#  NLP Assignment: Fine-Tuning a Transformer on Medical Question Pairs\n",
        "\n",
        "In this assignment, you will fine-tune a transformer model to classify whether pairs of medical questions are paraphrases of each other.\n",
        "\n",
        "Once you have everyting in place then before training, you should restart and change the runtime to TPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "3b638748",
      "metadata": {
        "id": "3b638748",
        "outputId": "7940fac8-8c21-40eb-a595-ede5f451e9de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/491.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m21.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/116.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# Install necessary libraries\n",
        "!pip install transformers datasets -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##TODO: set random seed to your BlugoldID\n",
        "seed=7835936"
      ],
      "metadata": {
        "id": "ajIUUTKzbh88"
      },
      "id": "ajIUUTKzbh88",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "93724b3f",
      "metadata": {
        "id": "93724b3f"
      },
      "source": [
        "## Step 1: Load the Dataset\n",
        "Use the `datasets` library to load the [curaihealth medical questions pairs](https://huggingface.co/datasets/curaihealth/medical_questions_pairs) dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b2c2c7f",
      "metadata": {
        "id": "0b2c2c7f"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "# TODO: Load the dataset\n",
        "dataset = ####\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b825eb7",
      "metadata": {
        "id": "7b825eb7"
      },
      "source": [
        "##  Train/Validation/Test Split\n",
        "The `medical_questions_pairs` dataset only provides a single training set. You need to create your own train, validation, and test sets.\n",
        "\n",
        "We'll split the dataset into:\n",
        "- **Train:** 80%\n",
        "- **Validation:** 10%\n",
        "- **Test:** 10%\n",
        "\n",
        "Use `train_test_split` from the `datasets` library to do this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc1d578e",
      "metadata": {
        "id": "fc1d578e"
      },
      "outputs": [],
      "source": [
        "from datasets import DatasetDict\n",
        "\n",
        "# Step 1: Split into train + temp (val + test)\n",
        "temp_split = dataset['train'].train_test_split(test_size=0.2, seed=seed)\n",
        "\n",
        "# Step 2: Split temp into validation + test (50/50 of temp = 10% each)\n",
        "val_test_split = temp_split['test'].train_test_split(test_size=0.5, seed=seed)\n",
        "\n",
        "# Step 3: Combine splits into a DatasetDict\n",
        "split_dataset = DatasetDict({\n",
        "    'train': temp_split['train'],\n",
        "    'validation': val_test_split['train'],\n",
        "    'test': val_test_split['test']\n",
        "})\n",
        "\n",
        "split_dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## TODO: find an index  so that the question pair has label 0\n",
        "idx_0=##\n",
        "split_dataset['train'][idx_0]\n"
      ],
      "metadata": {
        "id": "2fftS7AGX5PC"
      },
      "id": "2fftS7AGX5PC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## TODO: find an index so that the question pair has label 1\n",
        "idx_1=##\n",
        "split_dataset['train'][idx_1]"
      ],
      "metadata": {
        "id": "_4lgIlTpZJR2"
      },
      "id": "_4lgIlTpZJR2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "d57071c0",
      "metadata": {
        "id": "d57071c0"
      },
      "source": [
        "## Step 2: Explore and Preprocess\n",
        "Examine the fields. Tokenize question pairs using a pretrained tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f4ffad30",
      "metadata": {
        "id": "f4ffad30"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        " ##Choose a model checkpoint\n",
        "checkpoint = 'microsoft/MiniLM-L12-H384-uncased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "## Tokenization function\n",
        "def tokenize_fn(example):\n",
        "    return tokenizer(example['question_1'], example['question_2'], truncation=True, padding='max_length',max_length=256)\n",
        "\n",
        "##Apply to dataset\n",
        "tokenized = split_dataset.map(tokenize_fn, batched=True)\n",
        "tokenized"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb087f5b",
      "metadata": {
        "id": "eb087f5b"
      },
      "source": [
        "## Step 3: Load Model\n",
        "Load a model for sequence classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54a74fb3",
      "metadata": {
        "id": "54a74fb3"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification\n",
        "\n",
        "# TODO: Define the model with correct number of labels\n",
        "model = ##"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3099290f",
      "metadata": {
        "id": "3099290f"
      },
      "source": [
        "## Step 4: Define Training Arguments\n",
        "Use Hugging Face `TrainingArguments` to configure training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "713acd6c",
      "metadata": {
        "id": "713acd6c"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    logging_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir='./logs',\n",
        "    report_to=\"none\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "985e83eb",
      "metadata": {
        "id": "985e83eb"
      },
      "source": [
        "## Step 5: Define Trainer\n",
        "Set up the `Trainer` object and begin training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e71ffb0",
      "metadata": {
        "id": "9e71ffb0"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=1)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\"accuracy\": acc}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c00c4b79",
      "metadata": {
        "id": "c00c4b79"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer\n",
        "\n",
        "##Define the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized['train'],\n",
        "    eval_dataset=tokenized['validation'],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hNXLYwZXM7Ub",
      "metadata": {
        "id": "hNXLYwZXM7Ub"
      },
      "outputs": [],
      "source": [
        "# Start training\n",
        "##REMEMBER to set runtime type to GPU!\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20696227",
      "metadata": {
        "id": "20696227"
      },
      "source": [
        "## Step 6: Evaluation\n",
        "Evaluate and inspect results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "851ff034",
      "metadata": {
        "id": "851ff034"
      },
      "outputs": [],
      "source": [
        "##Evaluate the model\n",
        "metrics = trainer.evaluate()\n",
        "print(metrics)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save locally first\n",
        "model.save_pretrained(\"medical-question-model\")\n",
        "tokenizer.save_pretrained(\"medical-question-model\")\n",
        "\n",
        "## Push to hub\n",
        "## TODO: replace (your-username) with your huggingface user name\n",
        "model.push_to_hub(\"your-username/medical-question-model\")\n",
        "tokenizer.push_to_hub(\"your-username/medical-question-model\")"
      ],
      "metadata": {
        "id": "Me1el8OHRSnA"
      },
      "id": "Me1el8OHRSnA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### Training Accuracy"
      ],
      "metadata": {
        "id": "DGaBtH6Gc1SM"
      },
      "id": "DGaBtH6Gc1SM"
    },
    {
      "cell_type": "markdown",
      "id": "b573880c",
      "metadata": {
        "id": "b573880c"
      },
      "source": [
        "\n",
        "Now that training is complete, let's evaluate the model on the training set to report training accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02493441",
      "metadata": {
        "id": "02493441"
      },
      "outputs": [],
      "source": [
        "# Evaluate on training data\n",
        "train_metrics = trainer.evaluate(tokenized[\"train\"])\n",
        "print(f\"Training Accuracy: {train_metrics['eval_accuracy']:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate Custom Question Pairs"
      ],
      "metadata": {
        "id": "VGD4U3BDF2ve"
      },
      "id": "VGD4U3BDF2ve"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use this section to test your fine-tuned model on your own question pairs. This is useful for exploring how well the model generalizes to new examples outside the original dataset."
      ],
      "metadata": {
        "id": "aXqTxMaEGFFC"
      },
      "id": "aXqTxMaEGFFC"
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: replace (your-username) with your huggingface user name\n",
        "model_id = \"your-username/medical-question-model\""
      ],
      "metadata": {
        "id": "FEijsL5-FZsN"
      },
      "id": "FEijsL5-FZsN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code block below sets up a function to use your fine-tuned model on a pair of text fields."
      ],
      "metadata": {
        "id": "XhL_7-3uHGJ8"
      },
      "id": "XhL_7-3uHGJ8"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def evaluate_question_pair(question1, question2, model_id=model_id):\n",
        "    # Load model and tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(model_id)\n",
        "    model.eval()\n",
        "\n",
        "    # Determine max length\n",
        "    max_len = getattr(model.config, \"max_position_embeddings\", 512)\n",
        "\n",
        "    # Tokenize with explicit max_length\n",
        "    inputs = tokenizer(\n",
        "        question1, question2,\n",
        "        return_tensors=\"pt\",\n",
        "        truncation=True,\n",
        "        padding=True,\n",
        "        max_length=max_len\n",
        "    )\n",
        "\n",
        "    # Run through model\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "\n",
        "    predicted_class = torch.argmax(probs).item()\n",
        "    confidence = probs[0][predicted_class].item()\n",
        "    label_name = model.config.id2label.get(predicted_class, str(predicted_class))\n",
        "\n",
        "    print(f\"Q1: {question1}\")\n",
        "    print(f\"Q2: {question2}\")\n",
        "    print(f\"Predicted Label: {label_name} (Confidence: {confidence:.4f})\")\n",
        "\n",
        "    return predicted_class, confidence\n"
      ],
      "metadata": {
        "id": "y2o8g_IgFaU5"
      },
      "id": "y2o8g_IgFaU5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Example usage:\n",
        "\n",
        "evaluate_question_pair(\n",
        "    \"What are the symptoms of anemia?\",\n",
        "    \"Can being tired all the time mean I have anemia?\"\n",
        ")"
      ],
      "metadata": {
        "id": "HBJvcWegG3Gn"
      },
      "id": "HBJvcWegG3Gn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##Use seed to select two integers between 1 and 100 that are not equal\n",
        "##Reminder: seed is set above to be your Blugold ID\n",
        "\n",
        "import random\n",
        "\n",
        "# Use seed to select two integers between 1 and 100 that are not equal\n",
        "random.seed(seed)\n",
        "\n",
        "pick_1 = random.randint(1, 100)\n",
        "pick_2 = random.randint(1, 100)\n",
        "\n",
        "while pick_1 == pick_2:\n",
        "  pick_2 = random.randint(1, 100)\n",
        "\n",
        "print([pick_1, pick_2])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R9wlWFhMJljb",
        "outputId": "179911e7-3326-42e5-c45f-3ec47420637b"
      },
      "id": "R9wlWFhMJljb",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[82, 15]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "### Additional Question Set\n",
        "\n",
        "You can access a curated set of 100 medically-related questions here:  \n",
        "[📄 Medical_Questions.csv](https://github.com/alex-smith-uwec/NLP_Spring2025/blob/main/Medical_Questions.csv)\n",
        "\n",
        "These questions were **not part of the original training dataset**. They have been **synthetically generated** with care to ensure that they are medically relevant while **avoiding content that is gross, graphic, or sexual in nature**.\n",
        "\n",
        "You will use this file to generate rephrasings in the next part of the assignment.\n",
        "\n",
        "---\n",
        "\n",
        "###  Generate and Evaluate Rephrasings\n",
        "Look up the two rows in the spreadsheet file corresponding to your `pick_1` and `pick_2` to see which two questions are assigned to you.\n",
        "\n",
        "You’ve already selected two questions, `pick_1` and `pick_2`, from the dataset above. In this part of the assignment, you will create **two new rephrasings** for each of these selected questions:\n",
        "\n",
        "- **One rephrasing that preserves the original meaning**  \n",
        "- **One rephrasing that alters the original meaning**\n",
        "\n",
        "Then, you'll use your fine-tuned model to evaluate the similarity between the original and the rephrased versions.\n",
        "\n",
        " **Steps:**\n",
        "\n",
        "1. For each of `pick_1` and `pick_2`, write:\n",
        "   - One **meaning-preserving** rephrasing\n",
        "   - One **meaning-changing** rephrasing\n",
        "\n",
        "2. Use `evaluate_question_pair()` to compare:\n",
        "   - `pick_1` with its two rephrasings\n",
        "   - `pick_2` with its two rephrasings\n",
        "\n",
        "This means you'll make **four total calls** to `evaluate_question_pair`.\n",
        "\n",
        "Reflect on the model's behavior:\n",
        "- Does it correctly identify the rephrasing that preserves meaning?\n",
        "- Does it detect that the other rephrasing changes the intent?\n",
        "\n",
        "Show the four calls below in code blocks. Then add a text block for the reflection on model behavior."
      ],
      "metadata": {
        "id": "Ixb_BuXMKjN2"
      },
      "id": "Ixb_BuXMKjN2"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JmMQ_DV3KdrH"
      },
      "id": "JmMQ_DV3KdrH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2Z9QOUuUNMBo"
      },
      "id": "2Z9QOUuUNMBo",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}